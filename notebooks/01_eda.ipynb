{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c53e229",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b575adeb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Unable to find a usable engine; tried using: 'pyarrow', 'fastparquet'.\nA suitable version of pyarrow or fastparquet is required for parquet support.\nTrying to import the above resulted in these errors:\n - Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.\n - Missing optional dependency 'fastparquet'. fastparquet is required for parquet support. Use pip or conda to install fastparquet.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m FIGS_DIR\u001b[38;5;241m.\u001b[39mmkdir(parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Load cleaned news data\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m news_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCLEAN_DATA_PATH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m news_df\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[0;32m~/Desktop/fns_pid-week1/.venv/lib/python3.9/site-packages/pandas/io/parquet.py:653\u001b[0m, in \u001b[0;36mread_parquet\u001b[0;34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[0m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;129m@doc\u001b[39m(storage_options\u001b[38;5;241m=\u001b[39m_shared_docs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    501\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mread_parquet\u001b[39m(\n\u001b[1;32m    502\u001b[0m     path: FilePath \u001b[38;5;241m|\u001b[39m ReadBuffer[\u001b[38;5;28mbytes\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    511\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[1;32m    512\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    513\u001b[0m \u001b[38;5;124;03m    Load a parquet object from the file path, returning a DataFrame.\u001b[39;00m\n\u001b[1;32m    514\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    650\u001b[0m \u001b[38;5;124;03m    1    4    9\u001b[39;00m\n\u001b[1;32m    651\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 653\u001b[0m     impl \u001b[38;5;241m=\u001b[39m \u001b[43mget_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    655\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m use_nullable_dtypes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default:\n\u001b[1;32m    656\u001b[0m         msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    657\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe argument \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muse_nullable_dtypes\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is deprecated and will be removed \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    658\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min a future version.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    659\u001b[0m         )\n",
      "File \u001b[0;32m~/Desktop/fns_pid-week1/.venv/lib/python3.9/site-packages/pandas/io/parquet.py:68\u001b[0m, in \u001b[0;36mget_engine\u001b[0;34m(engine)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m     66\u001b[0m             error_msgs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m - \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(err)\n\u001b[0;32m---> 68\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     69\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to find a usable engine; \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     70\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtried using: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfastparquet\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     71\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA suitable version of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     72\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow or fastparquet is required for parquet \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     73\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msupport.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     74\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrying to import the above resulted in these errors:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     75\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_msgs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     76\u001b[0m     )\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m engine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m PyArrowImpl()\n",
      "\u001b[0;31mImportError\u001b[0m: Unable to find a usable engine; tried using: 'pyarrow', 'fastparquet'.\nA suitable version of pyarrow or fastparquet is required for parquet support.\nTrying to import the above resulted in these errors:\n - Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.\n - Missing optional dependency 'fastparquet'. fastparquet is required for parquet support. Use pip or conda to install fastparquet."
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Paths\n",
    "PROJECT_ROOT = Path(\"../\")  # notebook relative path\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "CLEAN_DATA_PATH = DATA_DIR / \"clean_news.parquet\"\n",
    "FIGS_DIR = PROJECT_ROOT / \"outputs/figs\"\n",
    "FIGS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load cleaned news data\n",
    "news_df = pd.read_parquet(CLEAN_DATA_PATH)\n",
    "news_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fb16b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "20438896",
   "metadata": {},
   "source": [
    "# Headline Length Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc900f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive stats\n",
    "stats = news_df[['headline_length_chars', 'headline_length_tokens']].describe()\n",
    "print(stats)\n",
    "\n",
    "# Histogram of headline lengths (chars)\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.hist(news_df['headline_length_chars'], bins=50, color='skyblue', edgecolor='black')\n",
    "plt.title(\"Headline Length (Characters)\")\n",
    "plt.xlabel(\"Characters\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.grid(axis='y', alpha=0.75)\n",
    "plt.savefig(FIGS_DIR / \"headline_length_chars_hist.png\")\n",
    "plt.show()\n",
    "\n",
    "# Histogram of headline lengths (tokens)\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.hist(news_df['headline_length_tokens'], bins=50, color='lightgreen', edgecolor='black')\n",
    "plt.title(\"Headline Length (Tokens)\")\n",
    "plt.xlabel(\"Tokens\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.grid(axis='y', alpha=0.75)\n",
    "plt.savefig(FIGS_DIR / \"headline_length_tokens_hist.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ec4de1",
   "metadata": {},
   "source": [
    "# Top Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeedf117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert headlines to lowercase\n",
    "headlines_lower = news_df['headline_lower'].fillna('')\n",
    "\n",
    "# Extract unigrams and bigrams appearing in at least 5 headlines\n",
    "vectorizer = CountVectorizer(ngram_range=(1,2), min_df=5, stop_words='english')\n",
    "X = vectorizer.fit_transform(headlines_lower)\n",
    "\n",
    "# Sum word counts\n",
    "sums = X.sum(axis=0)\n",
    "terms = [(term, sums[0, idx]) for term, idx in vectorizer.vocabulary_.items()]\n",
    "top_terms = sorted(terms, key=lambda x: x[1], reverse=True)[:30]\n",
    "\n",
    "# Display top 30 keywords/bigrams\n",
    "top_terms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b97f98",
   "metadata": {},
   "source": [
    "# Publisher Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b708b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "publisher_counts = news_df['publisher'].value_counts().head(20)\n",
    "print(publisher_counts)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10,5))\n",
    "publisher_counts.plot(kind='bar', color='orange', edgecolor='black')\n",
    "plt.title(\"Top 20 Publishers\")\n",
    "plt.ylabel(\"Number of Articles\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGS_DIR / \"top_20_publishers.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0cc56b",
   "metadata": {},
   "source": [
    "# Stock Ticker Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd826822",
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker_counts = news_df['stock'].value_counts().head(20)\n",
    "print(ticker_counts)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10,5))\n",
    "ticker_counts.plot(kind='bar', color='purple', edgecolor='black')\n",
    "plt.title(\"Top 20 Stock Tickers\")\n",
    "plt.ylabel(\"Number of Articles\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGS_DIR / \"top_20_stocks.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d73950",
   "metadata": {},
   "source": [
    "# Daily Article Trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e324dda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by date_only\n",
    "daily_counts = news_df.groupby('date_only').size()\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "daily_counts.plot(color='green', marker='o')\n",
    "plt.title(\"Daily Article Counts\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Number of Articles\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGS_DIR / \"daily_article_counts.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b935f7",
   "metadata": {},
   "source": [
    "# Rolling Spike Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96829eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "rolling_window = 7\n",
    "rolling_mean = daily_counts.rolling(rolling_window).mean()\n",
    "rolling_std = daily_counts.rolling(rolling_window).std()\n",
    "threshold = rolling_mean + 2*rolling_std\n",
    "\n",
    "spike_dates = daily_counts[daily_counts > threshold].index\n",
    "print(\"Spike Dates:\", spike_dates)\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "daily_counts.plot(label=\"Daily Count\")\n",
    "threshold.plot(label=\"Spike Threshold\", linestyle='--')\n",
    "plt.scatter(spike_dates, daily_counts[spike_dates], color='red', label='Spike')\n",
    "plt.title(\"Daily Article Counts with Spike Detection\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Number of Articles\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGS_DIR / \"daily_spikes.png\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
